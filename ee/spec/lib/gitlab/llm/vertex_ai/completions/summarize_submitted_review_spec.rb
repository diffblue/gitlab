# frozen_string_literal: true

require 'spec_helper'

RSpec.describe Gitlab::Llm::VertexAi::Completions::SummarizeSubmittedReview, feature_category: :code_review_workflow do
  let_it_be(:llm_bot) { create(:user, :llm_bot) }
  let_it_be(:project) { create(:project, :repository) }
  let_it_be(:merge_request) { create(:merge_request, source_project: project, target_project: project) }
  let_it_be(:mr_diff) { merge_request.merge_request_diff }
  let_it_be(:user) { merge_request.author }
  let_it_be(:review) { create(:review, merge_request: merge_request, author: user) }
  let_it_be(:note_1) { create(:note_on_merge_request, project: project, noteable: merge_request, review: review) }
  let_it_be(:note_2) { create(:diff_note_on_merge_request, project: project, noteable: merge_request, review: review) }

  let(:prompt_class) { Gitlab::Llm::Templates::SummarizeSubmittedReview }
  let(:options) do
    {
      diff_id: mr_diff.id,
      review_id: review.id,
      request_id: 'uuid'
    }
  end

  subject { described_class.new(prompt_class, options) }

  describe '#execute' do
    context 'when the chat client returns a successful response' do
      let(:example_answer) { 'Summary generated by AI' }

      let(:example_response) do
        {
          "predictions" => [
            {
              "candidates" => [
                {
                  "author" => "",
                  "content" => example_answer
                }
              ],
              "safetyAttributes" => {
                "categories" => ["Violent"],
                "scores" => [0.4000000059604645],
                "blocked" => false
              }
            }
          ]
        }
      end

      before do
        allow_next_instance_of(Gitlab::Llm::VertexAi::Client) do |client|
          allow(client).to receive(:chat).and_return(example_response.to_json)
        end
      end

      it 'stores the content from the AI response' do
        expect { subject.execute(user, merge_request, options) }
          .to change { mr_diff.merge_request_review_llm_summaries.count }
          .by(1)

        latest_review_llm_summary = mr_diff.merge_request_review_llm_summaries.last

        aggregate_failures do
          expect(latest_review_llm_summary.merge_request_diff).to eq(mr_diff)
          expect(latest_review_llm_summary.review).to eq(review)
          expect(latest_review_llm_summary.user).to eq(llm_bot)
          expect(latest_review_llm_summary.provider).to eq('vertex_ai')
          expect(latest_review_llm_summary.content).to eq('Summary generated by AI')
        end
      end

      it 'create a todo' do
        expect_next_instance_of(TodoService) do |svc|
          expect(svc).to receive(:review_submitted).with(review)
        end

        subject.execute(user, merge_request, options)
      end
    end

    context 'when the chat client returns an unsuccessful response' do
      let(:error) { { error: { message: 'Error' } } }

      before do
        allow_next_instance_of(Gitlab::Llm::VertexAi::Client) do |client|
          allow(client).to receive(:chat).and_return(error.to_json)
        end
      end

      it 'does not store the content' do
        expect { subject.execute(user, merge_request, options) }
          .not_to change { mr_diff.merge_request_review_llm_summaries.count }
      end

      it 'does not create a todo' do
        expect(TodoService).not_to receive(:new)

        subject.execute(user, merge_request, options)
      end
    end
  end
end
